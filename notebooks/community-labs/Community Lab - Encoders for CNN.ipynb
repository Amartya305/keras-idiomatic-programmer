{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composable \"Design Pattern\" for AutoML friendly models\n",
    "\n",
    "## Community Lab 1: Training Encoder for CNN\n",
    "\n",
    "### Objective\n",
    "\n",
    "To replace a traditional \"stem convolution group\" of higher input dimensionality with lower dimensionality encoding, learned from first training the dataset on an autoencoder. Goal is that by using a lower dimensionality encoding, one can substantially increase training time of a model.\n",
    "\n",
    "*Question*: Can one achieve the same accuracy as using the original input image?\n",
    "\n",
    "*Question*: How fast can we speed up training?\n",
    "\n",
    "### Approach\n",
    "\n",
    "We will use the composable design pattern, and prebuilt units from the Google Cloud AI Developer Relations repo: [Model Zoo](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo)\n",
    "\n",
    "If you are not familiar with the Composable design pattern, we recommemd you review the [ResNet](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo/resnet) model in our zoo. Then review the [AutoEncoder](https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/tree/master/zoo/autoencoder) model.\n",
    "\n",
    "We recommend a constant set for hyperparameters, where batch_size is 32 and initial learning rate is 0.001 -- but you may use any value for hyperparameters you prefer.\n",
    "\n",
    "We will use the metaparameters feature in the composable design pattern for the macro architecture search -- sort of a 'human assisted AutoML'.\n",
    "\n",
    "We recommend using a warmup training to find most optimal initialization of weights.\n",
    "\n",
    "### Reporting Findings\n",
    "\n",
    "You can contact us on your findings via the twitter account: @andrewferlitsch\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this notebook, we use the CIFAR-10 datasets which consist of images 32x32x3 for 10 classes -- but you may use any dataset you prefer.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Build and Train an AutoEncoder for CIFAR10 (or your dataset).\n",
    "\n",
    "2. Extract the pretrained Encoder network from the trained AutoEncoder.\n",
    "\n",
    "3. Preprocess the training and test data with the Encoder.\n",
    "\n",
    "4. Build a composable model for CIFAR10 using the Encoder embedding.\n",
    "\n",
    "5. Use warmup to initialize the weights on the model.\n",
    "\n",
    "6. Train the model with the encoded training set.\n",
    "\n",
    "7. Evaluate the model with the endoded test set.\n",
    "\n",
    "8. Repeat making macro architecture modifications to the AutoEncoder and/or model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Conv2DTranspose, ReLU, Add, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Dataset\n",
    "\n",
    "Load the dataset into memory as numpy arrays, and then normalize the image data (preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test  = (x_test / 255.0).astype(np.float32)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the AutoEncoder for CIFAR-10\n",
    "\n",
    "Now, let's build the AutoEncoder for the dataset.\n",
    "\n",
    "In our example, the dimensionality of the input (3072 pixels) is reduced down to 512 at the bottleneck layer (ReLU (None, 4, 4, 32))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autoencoder/autoencoder_c.py\n",
    "\n",
    "class AutoEncoder(object):\n",
    "    ''' Construct an AutoEncoder '''\n",
    "    # metaparameter: number of filters per layer\n",
    "    layers = [ {'n_filters': 64 }, { 'n_filters': 32 }, { 'n_filters': 16 } ]\n",
    "\n",
    "    input_shape=(32, 32, 3)\n",
    "\n",
    "    _model = None\n",
    "    init_weights = 'he_normal'\n",
    "    reg = None\n",
    "\n",
    "    def __init__(self, layers=None, input_shape=(32, 32, 3)):\n",
    "        ''' Construct an AutoEncoder\n",
    "            input_shape : input shape to the autoencoder\n",
    "            layers      : the number of filters per layer\n",
    "        '''\n",
    "        if layers is None:\n",
    "           layers = AutoEncoder.layers\n",
    "\n",
    "        # remember the layers\n",
    "        self.layers = layers\n",
    "\n",
    "        # remember the input shape\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "        encoder = AutoEncoder.encoder(inputs, layers=layers)\n",
    "        outputs = AutoEncoder.decoder(encoder, layers=layers)\n",
    "        self._model = Model(inputs, outputs)\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, _model):\n",
    "        self._model = _model\n",
    "\n",
    "    @staticmethod\n",
    "    def encoder(x, init_weights=None, **metaparameters):\n",
    "        ''' Construct the Encoder \n",
    "            x     : input to the encoder\n",
    "            layers: number of filters per layer\n",
    "        '''\n",
    "        layers = metaparameters['layers']\n",
    "\n",
    "        if init_weights is None:\n",
    "            init_weights = AutoEncoder.init_weights\n",
    "\n",
    "        # Progressive Feature Pooling\n",
    "        for layer in layers:\n",
    "            n_filters = layer['n_filters']\n",
    "            x = Conv2D(n_filters, (3, 3), strides=2, padding='same', kernel_initializer=init_weights,\n",
    "                       kernel_regularizer=AutoEncoder.reg)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = ReLU()(x)\n",
    "\n",
    "        # The Encoding\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def decoder(x, init_weights=None, **metaparameters):\n",
    "        ''' Construct the Decoder\n",
    "            x     : input to the decoder\n",
    "            layers: number of filters per layer\n",
    "        '''\n",
    "        layers = metaparameters['layers']\n",
    "\n",
    "        if init_weights is None:\n",
    "            init_weights = AutoEncoder.init_weights\n",
    "\n",
    "        # Progressive Feature Unpooling\n",
    "        for _ in range(len(layers)-1, 0, -1):\n",
    "            n_filters = layers[_]['n_filters']\n",
    "            x = Conv2DTranspose(n_filters, (3, 3), strides=2, padding='same', kernel_initializer=init_weights,\n",
    "                                kernel_regularizer=AutoEncoder.reg)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = ReLU()(x)\n",
    "\n",
    "        # Last unpooling and match shape to input\n",
    "        x = Conv2DTranspose(3, (3, 3), strides=2, padding='same', kernel_initializer=init_weights,\n",
    "                            kernel_regularizer=AutoEncoder.reg)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "\n",
    "        # The decoded image\n",
    "        return x\n",
    "\n",
    "    def compile(self, optimizer='adam'):\n",
    "        ''' Compile the model using Mean Square Error loss '''\n",
    "        self._model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    def extract(self):\n",
    "        ''' Extract the pretrained encoder\n",
    "        '''\n",
    "        # Get the trained weights from the autoencoder\n",
    "        weights = self._model.get_weights()\n",
    "\n",
    "        # Extract out the weights for just the encoder  (6 sets per layer)\n",
    "        encoder_weights = weights[0 : int((6 * len(self.layers)))]\n",
    "  \n",
    "        # Construct a copy the encoder\n",
    "        inputs = Input(self.input_shape)\n",
    "        outputs = self.encoder(inputs, layers=self.layers)\n",
    "        encoder = Model(inputs, outputs)\n",
    "\n",
    "        # Initialize the encoder with the pretrained weights\n",
    "        encoder.set_weights(encoder_weights)\n",
    "\n",
    "        return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(input_shape=(32, 32, 3), layers=[{'n_filters': 64}, {'n_filters': 32}, {'n_filters': 32}])\n",
    "autoencoder.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup Training for AutoEncoder\n",
    "\n",
    "Now let's find the best initialization of the encoder. We will do five separate draws from a random He Normal distribution -- but you can use more draws if you want to.\n",
    "\n",
    "For each draw, we will use a small subset of the training data (100 batches for 3200 images), a very low learning rate of 0.00001, and three epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "WARMUP_LR=0.00001 # The warmup learning rate\n",
    "\n",
    "models = []\n",
    "#  We will warmup train 5 instances of the non-compiled model.\n",
    "for _ in range(5):\n",
    "    warmup = AutoEncoder(input_shape=(32, 32, 3), layers=[{'n_filters': 64}, {'n_filters': 32}, {'n_filters': 32}])\n",
    "    \n",
    "    #  Compile the model, which will initialize the weights.\n",
    "    warmup.compile(optimizer=Adam(lr=WARMUP_LR))\n",
    "    \n",
    "    w_train = x_train[0:32 * 100]\n",
    "\n",
    "    #  Do a brief warmup training.\n",
    "    history = warmup.model.fit(w_train, w_train, epochs=3, verbose=1, batch_size=32, validation_split=0.1)\n",
    "    models.append((warmup, history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick best initialized model\n",
    "\n",
    "When completed, we will review the warmup history for each model instance, and use your judgement which draw (model instance) will give you the best training result (i.e., 'the winning ticket')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the AutoEncoder\n",
    "\n",
    "Let's now fully train the autoencoder on our image data for 20 epochs -- but you may choose to use more.\n",
    "\n",
    "*When using colab with runtime=GPU, this takes about 4 minutes*\n",
    "*You should see a validation accuracy ~80%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam')\n",
    "autoencoder.model.fit(x_train, x_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the accuracy is on the test (holdout) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.model.evaluate(x_test, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the pre-trained Encoder\n",
    "\n",
    "Next, we will extract from the pretrained encoder from our trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = autoencoder.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the CIFAR-10 Training Data\n",
    "\n",
    "Next, we will encode the higher dimensional training data (*x_train*) into the lower dimensional encoding (*e_train*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_train = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build mini-ResNet with Encoding as input (no stem convolution)\n",
    "\n",
    "Let's now use the composable design pattern for ResNet to build a mini-resnet model (*e_resnet*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from resnet/resnet_v2_c.py\n",
    "\n",
    "class ResNetV2(object):\n",
    "    \"\"\" Construct a Residual Convolution Network Network V2 \"\"\"\n",
    "    # Meta-parameter: list of groups: number of filters and number of blocks\n",
    "    groups = { 50 : [ { 'n_filters' : 64, 'n_blocks': 3 },\n",
    "                      { 'n_filters': 128, 'n_blocks': 4 },\n",
    "                      { 'n_filters': 256, 'n_blocks': 6 },\n",
    "                      { 'n_filters': 512, 'n_blocks': 3 } ],            # ResNet50\n",
    "               101: [ { 'n_filters' : 64, 'n_blocks': 3 },\n",
    "                      { 'n_filters': 128, 'n_blocks': 4 },\n",
    "                      { 'n_filters': 256, 'n_blocks': 23 },\n",
    "                      { 'n_filters': 512, 'n_blocks': 3 } ],            # ResNet101\n",
    "               152: [ { 'n_filters' : 64, 'n_blocks': 3 },\n",
    "                      { 'n_filters': 128, 'n_blocks': 8 },\n",
    "                      { 'n_filters': 256, 'n_blocks': 36 },\n",
    "                      { 'n_filters': 512, 'n_blocks': 3 } ]             # ResNet152\n",
    "             }\n",
    "    init_weights = 'he_normal'\n",
    "    reg=l2(0.001)\n",
    "    _model = None\n",
    "\n",
    "    def __init__(self, n_layers, input_shape=(224, 224, 3), n_classes=1000):\n",
    "        \"\"\" Construct a Residual Convolutional Neural Network V2\n",
    "            n_layers   : number of layers\n",
    "            input_shape: input shape\n",
    "            n_classes  : number of output classes\n",
    "        \"\"\"\n",
    "        # predefined\n",
    "        if isinstance(n_layers, int):\n",
    "            if n_layers not in [50, 101, 152]:\n",
    "                raise Exception(\"ResNet: Invalid value for n_layers\")\n",
    "            groups = self.groups[n_layers]\n",
    "        # user defined\n",
    "        else:\n",
    "            groups = n_layers\n",
    "\n",
    "        # The input tensor\n",
    "        inputs = Input(input_shape)\n",
    "\n",
    "        # The stem convolutional group\n",
    "        x = self.stem(inputs)\n",
    "\n",
    "        # The learner\n",
    "        x = self.learner(x, groups=groups)\n",
    "\n",
    "        # The classifier \n",
    "        outputs = self.classifier(x, n_classes)\n",
    "\n",
    "        # Instantiate the Model\n",
    "        self._model = Model(inputs, outputs)\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, _model):\n",
    "        self._model = _model\n",
    "\n",
    "    def stem(self, inputs):\n",
    "        \"\"\" Construct the Stem Convolutional Group \n",
    "            inputs : the input vector\n",
    "        \"\"\"\n",
    "        # The 224x224 images are zero padded (black - no signal) to be 230x230 images prior to the first convolution\n",
    "        x = ZeroPadding2D(padding=(3, 3))(inputs)\n",
    "    \n",
    "        # First Convolutional layer uses large (coarse) filter\n",
    "        x = Conv2D(64, (7, 7), strides=(2, 2), padding='valid', use_bias=False, \n",
    "                   kernel_initializer=self.init_weights, kernel_regularizer=self.reg)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "    \n",
    "        # Pooled feature maps will be reduced by 75%\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "        return x\n",
    "\n",
    "    def learner(self, x, **metaparameters):\n",
    "        \"\"\" Construct the Learner\n",
    "            x     : input to the learner\n",
    "            groups: list of groups: number of filters and blocks\n",
    "        \"\"\"\n",
    "        groups = metaparameters['groups']\n",
    "\n",
    "        # First Residual Block Group (not strided)\n",
    "        x = ResNetV2.group(x, strides=(1, 1), **groups.pop(0))\n",
    "\n",
    "        # Remaining Residual Block Groups (strided)\n",
    "        for group in groups:\n",
    "            x = ResNetV2.group(x, **group)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def group(x, strides=(2, 2), init_weights=None, **metaparameters):\n",
    "        \"\"\" Construct a Residual Group\n",
    "            x         : input into the group\n",
    "            strides   : whether the projection block is a strided convolution\n",
    "            n_filters : number of filters for the group\n",
    "            n_blocks  : number of residual blocks with identity link\n",
    "        \"\"\"\n",
    "        n_blocks  = metaparameters['n_blocks']\n",
    "\n",
    "        # Double the size of filters to fit the first Residual Group\n",
    "        x = ResNetV2.projection_block(x, strides=strides, init_weights=init_weights, **metaparameters)\n",
    "\n",
    "        # Identity residual blocks\n",
    "        for _ in range(n_blocks):\n",
    "            x = ResNetV2.identity_block(x, init_weights=init_weights, **metaparameters)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def identity_block(x, init_weights=None, **metaparameters):\n",
    "        \"\"\" Construct a Bottleneck Residual Block with Identity Link\n",
    "            x        : input into the block\n",
    "            n_filters: number of filters\n",
    "            reg      : kernel regularizer\n",
    "        \"\"\"\n",
    "        n_filters = metaparameters['n_filters']\n",
    "        if 'reg' in metaparameters:\n",
    "            reg = metaparameters['reg']\n",
    "        else:\n",
    "            reg = ResNetV2.reg\n",
    "\n",
    "        if init_weights is None:\n",
    "            init_weights = ResNetV2.init_weights\n",
    "    \n",
    "        # Save input vector (feature maps) for the identity link\n",
    "        shortcut = x\n",
    "    \n",
    "        ## Construct the 1x1, 3x3, 1x1 convolution block\n",
    "    \n",
    "        # Dimensionality reduction\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv2D(n_filters, (1, 1), strides=(1, 1), use_bias=False, \n",
    "                   kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
    "\n",
    "        # Bottleneck layer\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv2D(n_filters, (3, 3), strides=(1, 1), padding=\"same\", use_bias=False, \n",
    "                   kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
    "\n",
    "        # Dimensionality restoration - increase the number of output filters by 4X\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv2D(n_filters * 4, (1, 1), strides=(1, 1), use_bias=False, \n",
    "                   kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
    "\n",
    "        # Add the identity link (input) to the output of the residual block\n",
    "        x = Add()([shortcut, x])\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def projection_block(x, strides=(2,2), init_weights=None, **metaparameters):\n",
    "        \"\"\" Construct a Bottleneck Residual Block of Convolutions with Projection Shortcut\n",
    "            Increase the number of filters by 4X\n",
    "            x        : input into the block\n",
    "            strides  : whether the first convolution is strided\n",
    "            n_filters: number of filters\n",
    "            reg      : kernel regularizer\n",
    "        \"\"\"\n",
    "        n_filters = metaparameters['n_filters']\n",
    "        if 'reg' in metaparameters:\n",
    "            reg = metaparameters['reg']\n",
    "        else:\n",
    "            reg = ResNetV2.reg\n",
    "\n",
    "        if init_weights is None:\n",
    "            init_weights = ResNetV2.init_weights\n",
    "\n",
    "        # Construct the projection shortcut\n",
    "        # Increase filters by 4X to match shape when added to output of block\n",
    "        shortcut = BatchNormalization()(x)\n",
    "        shortcut = Conv2D(4 * n_filters, (1, 1), strides=strides, use_bias=False, \n",
    "                          kernel_initializer=init_weights, kernel_regularizer=reg)(shortcut)\n",
    "\n",
    "        ## Construct the 1x1, 3x3, 1x1 convolution block\n",
    "    \n",
    "        # Dimensionality reduction\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv2D(n_filters, (1, 1), strides=(1,1), use_bias=False, \n",
    "                   kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
    "\n",
    "        # Bottleneck layer\n",
    "        # Feature pooling when strides=(2, 2)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv2D(n_filters, (3, 3), strides=strides, padding='same', use_bias=False, \n",
    "                   kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
    "\n",
    "        # Dimensionality restoration - increase the number of filters by 4X\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv2D(4 * n_filters, (1, 1), strides=(1, 1), use_bias=False, \n",
    "                   kernel_initializer=init_weights, kernel_regularizer=reg)(x)\n",
    "\n",
    "        # Add the projection shortcut to the output of the residual block\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    def classifier(self, x, n_classes):\n",
    "        \"\"\" Construct the Classifier Group \n",
    "            x         : input to the classifier\n",
    "            n_classes : number of output classes\n",
    "        \"\"\"\n",
    "        # Pool at the end of all the convolutional residual blocks\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "        # Final Dense Outputting Layer for the outputs\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer=self.init_weights, kernel_regularizer=self.reg)(x)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "# Encoded Input (no stem)\n",
    "inputs = Input((4, 4, 32))\n",
    "\n",
    "# Learner\n",
    "# Residual group: 2 blocks, 64 filters\n",
    "# Residual group: 1 blocks, 128 filters\n",
    "x = ResNetV2.group(inputs, n_blocks=2, n_filters=64)\n",
    "x = ResNetV2.group(x, n_blocks=1, n_filters=128)\n",
    "\n",
    "# Classifier\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "e_resnet = Model(inputs, outputs)\n",
    "e_resnet.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "e_resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Let's now train our mini-resnet model (*e_resnet*) with the encoded training data (*e_train*).\n",
    "\n",
    "*When using colab with runtime=GPU, this takes about 4 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_resnet.fit(e_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "Let's convert our test (holdout) data into an encoding (*e_test*) using our pretrained encoder (*encoder*), and evaluate our model (*e_resnet*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_test = encoder.predict(x_test)\n",
    "e_resnet.evaluate(e_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "If you followed this lab as-is, our encoded model overfits the encoded training data, and plateaus on accuracy on the encoded test data at ~61% (50% with V1).\n",
    "\n",
    "Think how you can modify this experiment, to meet the objectives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
